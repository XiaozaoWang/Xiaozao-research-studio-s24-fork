# Opinionated

## Abstract

The focus of my project is the use of AI for moderation and decision-making processes. I specifically aim at understanding and questioning the belief that algorithms can put an end to human bias in decision-making. Bias in AI can exist in the training model due to the dataset or the screener based on intended implementation. 
This project is developed as a part of Generative Research 2.0, a tool for addressing bias in creative research.

## Objectives

I intend to develop a guide to understanding bias in AI models with examples. These are a few example questions I hope to answer through the guide: What is bias?; Where does it exist in AI?; Biases in textual models versus image models; Details about transparency in models and datasets, How are ML models moderated?; What is the future of moderation through AI?; What are other human interventions to mitigate algorithmic bias?
I hope to adequately make accessible methods to mitigate bias for those interested in implementing their own and existing ML models.

## References

- A People's Guide to AI - Mimi Onuoha
- {class} on consequences in algorithmic classification - V2_Publishing (various authors)
- Field Guide to Address Bias in Datasets - Govind Nagubandi


## Methodology

Secondary research through papers and books; 
Qualitative research on bias in ML;
Case studies/examples of how to self assess bias while making datasets 

## Challenges

Primary Research; User Testing

## Final Deliverable

Develop a self-assessment tool that can be used as a part of the ml5.js library
Draft paper on GenResearch2.0

## Timeline

- Jan 31 - Feb 14: Conduct preliminary research and gather resources. Finalize project plan and post to class GitHub.
- Feb 14 - Feb 28: What is bias?
- Feb 28 - Mar 13: Primary/Secondary Research
- Mar 13 - Mar 27: Examining Current Models
- Mar 27 - Apr 10: Self-Assessment Tool/User Testing
- Apr 10 - Apr 17: Prepare final presentation.
